from langgraph.graph import END, START, StateGraph
from langgraph.types import Send, interrupt, Command
from typing import TypedDict
import subprocess
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import textwrap
from langchain.chat_models import init_chat_model
from typing_extensions import Annotated
import operator
import base64
from langgraph.checkpoint.memory import InMemorySaver


load_dotenv()

BASE_DIR = Path(__file__).parent
video_path = BASE_DIR / "video.mp4"

client = OpenAI()
llm = init_chat_model("openai:gpt-4o-mini")
memory = InMemorySaver()


class State(TypedDict):
    video_file: str
    audio_file: str
    transcript: str
    summaries: Annotated[list[str], operator.add]
    thumbnail_prompts: Annotated[list[str], operator.add]
    thumbnail_sketches: Annotated[list[str], operator.add]
    final_summary: str
    user_feedback: str
    chosen_prompt: str


def extract_audio(state: State):
    video_path = Path(state["video_file"])
    output_file = video_path.with_suffix(".mp3")

    command = [
        "ffmpeg",
        "-i",
        str(video_path),
        "-filter:a",
        "atempo=2.0",
        "-y",  # overwrite if exists
        str(output_file),
    ]

    subprocess.run(command, check=True)
    return {"audio_file": str(output_file)}


def transcribe_audio(state: State):
    with open(state["audio_file"], "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            response_format="text",
            file=audio_file,
            language="en",
            prompt="Korea, South",
        )

    return {"transcript": transcript}


def transcript_tokenizer(state: State):
    transcript = state["transcript"]
    chunks = []

    for i, chunk in enumerate(textwrap.wrap(transcript, 500)):
        chunks.append({"id": i + 1, "chunk": chunk})

    return [Send("summarize_chunk", chunk) for chunk in chunks]


def summarize_chunk(chunk):
    chunk_id = chunk["id"]
    chunk = chunk["chunk"]

    response = llm.invoke(
        f"""
        Please summarize the following text.

        Text: {chunk}
        """
    )

    summary = f"[Chunk {chunk_id}] {response.content}"

    return {"summaries": [summary]}


def mega_summary(state: State):
    all_summaries = "\n".join(state["summaries"])

    prompt = f"""
        You are given multiple summaries of different chunks from a video transcription.
        Please create a comprehensive final summary that combines all the key points.
        Individual summaries:
        {all_summaries}
    """

    response = llm.invoke(prompt)

    return {"final_summary": response.content}


def dispatch_artists(state: State):
    return [
        Send("generate_thumbnails", {"id": i, "summary": state["final_summary"]})
        for i in [1, 2, 3]
    ]


def generate_thumbnails(args):
    id = args["id"]
    summary = args["summary"]

    prompt = f"""
    Based on this video summary, create a detailed visual prompt for a YouTube thumbnail.

    Create a detailed prompt for generating a thumbnail image that would attract viewers. Include:
        - Main visual elements
        - Color scheme
        - Text overlay suggestions
        - Overall composition
    
    Summary: {summary}
    """

    response = llm.invoke(prompt)

    thumbnail_prompt = response.content

    result = client.images.generate(
        model="gpt-image-1",
        prompt=thumbnail_prompt,
        quality="low",
        moderation="low",
        size="auto",
    )

    image_bytes = base64.b64decode(result.data[0].b64_json)
    fileName = f"thumbnail_{id}.jpg"

    with open(fileName, "wb") as file:
        file.write(image_bytes)

    return {"thumbnail_prompts": [thumbnail_prompt], "thumbnail_sketches": [fileName]}


def human_feedback(state: State):
    answer = interrupt(
        {
            "chosen_thumbnail": "Which thumbnail do you like the most?",
            "feedback": "Provide any feedback or changes you'd like for the final thumbnail.",
        }
    )
    user_feedback = answer["user_feedback"]
    chosen_prompt = answer["chosen_prompt"]

    return {
        "user_feedback": user_feedback,
        "chosen_prompt": state["thumbnail_prompts"][chosen_prompt - 1],
    }


def generate_hd_thumbnail(state: State):
    chosen_prompt = state["chosen_prompt"]
    user_feedback = state["user_feedback"]

    prompt = f"""
    You are a professional YouTube thumbnail designer. Take this original thumbnail prompt and create an enhanced version that incorporates the user's specific feedback.

    ORIGINAL PROMPT:
    {chosen_prompt}

    USER FEEDBACK TO INCORPORATE:
    {user_feedback}

    Create an enhanced prompt that:
        1. Maintains the core concept from the original prompt
        2. Specifically addresses and implements the user's feedback requests
        3. Adds professional YouTube thumbnail specifications:
            - High contrast and bold visual elements
            - Clear focal points that draw the eye
            - Professional lighting and composition
            - Optimal text placement and readability with generous padding from edges
            - Colors that pop and grab attention
            - Elements that work well at small thumbnail sizes
            - IMPORTANT: Always ensure adequate white space/padding between any text and the image borders
    """

    response = llm.invoke(prompt)
    final_thumbnail_prompt = response.content

    result = client.images.generate(
        model="gpt-image-1",
        prompt=final_thumbnail_prompt,
        quality="high",
        moderation="low",
        size="auto",
    )

    image_bytes = base64.b64decode(result.data[0].b64_json)

    with open("thumbnail_final.jpg", "wb") as file:
        file.write(image_bytes)


graph_builder = StateGraph(State)

graph_builder.add_node("extract_audio", extract_audio)
graph_builder.add_node("transcribe_audio", transcribe_audio)
graph_builder.add_node("summarize_chunk", summarize_chunk)
graph_builder.add_node("mega_summary", mega_summary)
graph_builder.add_node("generate_thumbnails", generate_thumbnails)
graph_builder.add_node("human_feedback", human_feedback)
graph_builder.add_node("generate_hd_thumbnail", generate_hd_thumbnail)


graph_builder.add_edge(START, "extract_audio")
graph_builder.add_edge("extract_audio", "transcribe_audio")
# [] is for split into many chunks and parallel execution
graph_builder.add_conditional_edges(
    "transcribe_audio", transcript_tokenizer, ["summarize_chunk"]
)
graph_builder.add_edge("summarize_chunk", "mega_summary")
graph_builder.add_conditional_edges(
    "mega_summary", dispatch_artists, ["generate_thumbnails"]
)
graph_builder.add_edge("generate_thumbnails", "human_feedback")
graph_builder.add_edge("human_feedback", "generate_hd_thumbnail")
graph_builder.add_edge("generate_hd_thumbnail", END)


graph = graph_builder.compile(name="youtube_thumbnail", checkpointer=memory)

config = {"configurable": {"thread_id": "1"}}

final_state = graph.invoke({"video_file": str(video_path)}, config=config)

print("Waiting for human feedback...")

chosen_prompt = int(input("Which thumbnail do you like the most? (Enter number): "))
user_feedback = input("Provide any feedback or changes you'd like: ")
user_response = {"user_feedback": user_feedback, "chosen_prompt": chosen_prompt}

result = graph.invoke(
    Command(resume=user_response),
    config=config,
)
